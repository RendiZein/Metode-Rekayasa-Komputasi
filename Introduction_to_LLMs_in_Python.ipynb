{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOjHNHJ1gDsmPZJ5YT9Q/Fe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RendiZein/Metode-Rekayasa-Komputasi/blob/main/Introduction_to_LLMs_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Large Language Models (LLMs) Landscape"
      ],
      "metadata": {
        "id": "JV51VCL36Ffn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9WQKIvF4NVn"
      },
      "outputs": [],
      "source": [
        "# Import the function for loading Hugging Face pipelines\n",
        "from transformers import pipeline\n",
        "\n",
        "prompt = \"The food was good, but service at the restaurant was a bit slow\"\n",
        "\n",
        "# Load the pipeline for sentiment classification\n",
        "classifier = pipeline('text-classification', model=model_name)\n",
        "\n",
        "# Pass the customer review to the model for prediction\n",
        "prediction = classifier(prompt)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model pipeline for text summarization\n",
        "summarizer = pipeline('summarization', model=model_name)\n",
        "\n",
        "# Pass the long text to the model to summarize it\n",
        "outputs = summarizer(long_text, max_length=50)\n",
        "\n",
        "# Access and print the summarized text in the outputs variable\n",
        "print(outputs[0]['summary_text'])"
      ],
      "metadata": {
        "id": "TSAknyJ19oNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model pipeline for question-answering\n",
        "qa_model = pipeline(\"question-answering\")\n",
        "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
        "\n",
        "# Pass the necessary inputs to the LLM pipeline for question-answering\n",
        "outputs = qa_model(question=question, context=context)\n",
        "\n",
        "# Access and print the answer\n",
        "print(outputs['answer'])"
      ],
      "metadata": {
        "id": "iGx4q_Mw9nDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set transformer model hyperparameters\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "\n",
        "# Create the transformer model and assign hyperparameters\n",
        "model = nn.Transformer(\n",
        "    d_model=d_model,\n",
        "    nhead=n_heads,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    num_decoder_layers=num_decoder_layers\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "8kB3oh0sAPRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Transformer Architecture"
      ],
      "metadata": {
        "id": "M_OzU3qZuHE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass an appropriate PyTorch class\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_length):\n",
        "        super(PositionalEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Initialize the positional encoding matrix\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        # Calculate and assign position encodings to the matrix\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    # Update the embeddings tensor adding the positional encodings\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "Z9iQnuz0uGqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is positional encoding added to token embeddings?\n",
        "\n",
        "a. To accelerate the training process\n",
        "\n",
        "**b.To include sequence order information into the models**\n",
        "\n",
        "c.To outperform RNN models\n",
        "\n",
        "d.To remove long-range dependencies"
      ],
      "metadata": {
        "id": "PaoC7anruUsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Set the number of attention heads\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\t\t# Set up the linear transformations\n",
        "        self.query_linear = nn.Linear(d_model,d_model)\n",
        "        self.key_linear = nn.Linear(d_model,d_model)\n",
        "        self.value_linear = nn.Linear(d_model,d_model)\n",
        "        self.output_linear = nn.Linear(d_model,d_model)"
      ],
      "metadata": {
        "id": "C585STN2ujZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_heads(self, x, batch_size):\n",
        "    # Split the sequence embeddings in x across the attention heads\n",
        "    x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "    return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)"
      ],
      "metadata": {
        "id": "6jjqb3devzCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_attention(self, query, key, mask=None):\n",
        "    # Compute dot-product attention scores\n",
        "    scores = torch.matmul(query, key.permute(1, 2, 0))\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "    # Normalize attention scores into attention weights\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    return attention_weights"
      ],
      "metadata": {
        "id": "sKZ0Kk6rxku1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, query, key, value, mask=None):\n",
        "    batch_size = query.size(0)\n",
        "\n",
        "    query = self.split_heads(self.query_linear(query), batch_size)\n",
        "    key = self.split_heads(self.key_linear(key), batch_size)\n",
        "    value = self.split_heads(self.value_linear(value), batch_size)\n",
        "\n",
        "    attention_weights = self.compute_attention(query, key, mask)\n",
        "\n",
        "    # Multiply attention weights by values, concatenate and linearly project outputs\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "    return self.output_linear(output)"
      ],
      "metadata": {
        "id": "KNLy20SdxoHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
        "        # Define a stack of multiple encoder layers\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    # Complete the forward pass method\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class ClassifierHead(nn.Module):\n",
        "    def __init__(self, d_model, num_classes):\n",
        "        super(ClassifierHead, self).__init__()\n",
        "        # Add linear layer for multiple-class classification\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.fc(x[:, 0, :])\n",
        "        # Obtain log class probabilities upon raw outputs\n",
        "        return F.log_softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "YA88pajb09_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Initialize the causal (masked) self-attention and cross-attention\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
        "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
        "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "pzNPdaUABZ9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize positional encoding layer and stack of EncoderLayer modules\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass the sequence through each layer in the encoder\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize the encoder stack of the Transformer\n",
        "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        return encoder_output"
      ],
      "metadata": {
        "id": "rhA5MiDvF4L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Harnessing Pre-trained LLMs"
      ],
      "metadata": {
        "id": "GVTz0PTT2Lj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarizing a product opinion"
      ],
      "metadata": {
        "id": "dDbPiOcR2kNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of instances: {len(dataset['train'])}\")\n",
        "\n",
        "# Show the names of features in the training fold of the dataset\n",
        "print(f\"Feature names: {dataset['train'].column_names}\")\n",
        "\n",
        "# Encode the input example, obtain the summary, and decode it\n",
        "example = dataset['train'][-2]['review_sents']\n",
        "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "summary_ids = model.generate(input_ids, max_length=150)\n",
        "summary = tokenizer.decode(\n",
        "  summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\n",
        "print(\"\\nGenerated Summary: \\n\", summary)"
      ],
      "metadata": {
        "id": "Vxr9bYjW2MPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Spanish phrasebook mission"
      ],
      "metadata": {
        "id": "88yD1siW2n30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "\n",
        "# Load the tokenizer and the model checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
        "\n",
        "# Encode the inputs, generate translations, decode, and print them\n",
        "for english_input in english_inputs:\n",
        "    input_ids = tokenizer.encode(english_input, return_tensors=\"pt\")\n",
        "    translated_ids = model.generate(input_ids)\n",
        "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
      ],
      "metadata": {
        "id": "7krmNYEW2osK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and inspect a QA dataset"
      ],
      "metadata": {
        "id": "x9rQ05-Z5rOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a specific subset of the dataset\n",
        "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
        "\n",
        "question = mlqa[\"test\"][\"question\"][0]\n",
        "context = mlqa[\"test\"][\"context\"][0]\n",
        "print(\"Question: \", question)\n",
        "print(\"Context: \", context)\n",
        "\n",
        "# Initialize the tokenizer using the model checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepset/minilm-uncased-squad2\")\n",
        "\n",
        "# Tokenize the inputs returning the result as tensors\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "print(\"First five encoded tokens: \", inputs[\"input_ids\"][0][:5])"
      ],
      "metadata": {
        "id": "Yl82Onlj5r3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract and decode the answer"
      ],
      "metadata": {
        "id": "wtuanHLu5tGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM upon the model checkpoint\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Forward-pass the input through the model\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "# Get the most likely start and end answer position from the raw LLM outputs\n",
        "start_idx = torch.argmax(outputs.start_logits)\n",
        "end_idx = torch.argmax(outputs.end_logits) + 1\n",
        "\n",
        "# Access the tokenized inputs tensor to get the answer span\n",
        "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
        "\n",
        "# Decode the answer span to get the extracted answer text\n",
        "answer = tokenizer.decode(answer_span)\n",
        "print(\"Answer: \", answer)"
      ],
      "metadata": {
        "id": "jPBHYD7r5wEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning preparations"
      ],
      "metadata": {
        "id": "Uz5CUjsQ9HAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained LLM, specifying its use for binary classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Set up training arguments with a batch size of 8 per GPU and 5 epochs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./smaller_bert_finetuned\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        ")\n",
        "# Set up trainer, assigning previously set up training arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        ")"
      ],
      "metadata": {
        "id": "LYfJUc279Gr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The inside-out LLM"
      ],
      "metadata": {
        "id": "KpJgh0wZ9KI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the trainer and assign a training and validation set to it\n",
        "trainer = Trainer(model=model, args=training_args,\n",
        "    \t\t\tcompute_metrics=compute_metrics,\n",
        "    \t\t\ttrain_dataset=emotions_encoded[\"train\"],\n",
        "    \t\t\teval_dataset=emotions_encoded[\"validation\"],\n",
        "    \t\t\ttokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Training loop to fine-tune the model\n",
        "# trainer.train()\n",
        "\n",
        "input_texts = [\"It's dark and rainy outside\", \"I love penguins!\"]\n",
        "\n",
        "# Tokenize the input sequences and pass them to the model\n",
        "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    outputs  = model(**inputs)\n",
        "\n",
        "# Obtain class labels from raw predictions\n",
        "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "\n",
        "for i, predicted_label in enumerate(predicted_labels):\n",
        "    print(f\"\\n Input Text {i + 1}: {input_texts[i]}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "yjiidDln9MXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating and Leveraging LLMs in the Real World"
      ],
      "metadata": {
        "id": "lS1BThybXhn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating accuracy"
      ],
      "metadata": {
        "id": "VfpXNCMPXjnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the four input texts (without labels) to the pipeline\n",
        "predictions = sentiment_analysis([example[\"text\"] for example in test_examples])\n",
        "\n",
        "true_labels = [example[\"label\"] for example in test_examples]\n",
        "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
        "\n",
        "# Load the accuracy metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "result = accuracy.compute(references=true_labels, predictions=predicted_labels)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "7uiTQQLafGNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond accuracy: describing metrics"
      ],
      "metadata": {
        "id": "Jkb1XU0PXjY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the accuracy, precision, recall and F1 score metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# Obtain a description of each metric\n",
        "print(help(accuracy))\n",
        "print(help(precision))\n",
        "print(help(recall))\n",
        "print(help(f1))"
      ],
      "metadata": {
        "id": "eGQaUuOSfJ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond accuracy: using metrics"
      ],
      "metadata": {
        "id": "oXw41qObfI9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# Pass the examples to the pipeline, and obtain a list predicted labels\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "predictions = sentiment_analysis([example for example in test_examples])\n",
        "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
        "\n",
        "# Compute the metrics by comparing real and predicted labels\n",
        "print(precision.compute(references=test_labels, predictions=predicted_labels))\n",
        "print(recall.compute(references=test_labels, predictions=predicted_labels))\n",
        "print(f1.compute(references=test_labels, predictions=predicted_labels))"
      ],
      "metadata": {
        "id": "ZI8TMdodXivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexed about 2030"
      ],
      "metadata": {
        "id": "PMTCXb2hn77q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the prompt, generate text and decode it\n",
        "prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(prompt_ids, max_length=20)\n",
        "generated_text = tokenizer.decode(\n",
        "  output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Text: \", generated_text)\n",
        "\n",
        "# Load and compute the perplexity score\n",
        "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
        "results = perplexity.compute(model_id='gpt2',\n",
        "                             predictions=generated_text)\n",
        "print(\"Perplexity: \", results['mean_perplexity'])"
      ],
      "metadata": {
        "id": "J5Bkg6G4oD8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A feast of LLM metrics"
      ],
      "metadata": {
        "id": "U5H25vlsn9Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the rouge metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
        "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
        "\n",
        "# Calculate the rouge scores between the predicted and reference summaries\n",
        "results = rouge.compute(predictions=predictions,                     references=references)\n",
        "print(\"ROUGE results: \", results)"
      ],
      "metadata": {
        "id": "rwoc8oOaoGIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "llm_outputs = [\"He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures and practicing the deeds he had read about in chivalric tales.\"]\n",
        "references = [\"He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures, and enacting the heroic deeds he had encountered in tales of chivalry.\"]\n",
        "\n",
        "# Compute and print the METEOR score\n",
        "results = meteor.compute(predictions=llm_outputs, references=references)\n",
        "print(\"Meteor: \", results)"
      ],
      "metadata": {
        "id": "q01gCvSdoTrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match = evaluate.load(\"exact_match\")\n",
        "\n",
        "predictions = [\"The cat sat on the mat.\", \"Theaters are great.\", \"It's like comparing oranges and apples.\"]\n",
        "references = [\"The cat sat on the mat?\", \"Theaters are great.\", \"It's like comparing apples and oranges.\"]\n",
        "\n",
        "# Compute the exact match and print the results\n",
        "results = exact_match.compute(predictions=predictions, references=references)\n",
        "print(\"EM results: \", results)"
      ],
      "metadata": {
        "id": "QM0G0q6eoU9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU-proof translations"
      ],
      "metadata": {
        "id": "_aP9qGCwoCbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Translate the first input sentence\n",
        "translated_output = translator(input_sentence_1)\n",
        "\n",
        "translated_sentence = translated_output[0]['translation_text']\n",
        "\n",
        "print(\"Translated:\", translated_sentence)\n",
        "\n",
        "# Calculate BLEU metric for translation quality\n",
        "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "hAdAzC8soNnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "translated_outputs = translator(input_sentences_2)\n",
        "\n",
        "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "results = bleu.compute(predictions=predictions, references=references_2)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "kwDN6NxyoMCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Celt-MfcoKe8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9rT644vn85q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}